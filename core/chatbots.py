from .config import client, client_PEDRO
import time
import json
import plotly.express as px
from DB.dbInterface import *
from openai import OpenAI


# Funci√≥n para analizar si el prompt del usuario requiere estad√≠sticas
# Se consulta a Claude para determinar si la pregunta necesita gr√°ficos o tablas
def analyze_prompt_for_statistics(prompt):
    """
    Usa Claude para analizar si el prompt necesita generaci√≥n de estad√≠sticas.
    """
    analysis_prompt = f"""
    Analiza la siguiente consulta y responde con 'SI' si requiere datos estad√≠sticos, 
    como gr√°ficos, tablas o estad√≠sticas generales. Si no se necesitan estad√≠sticas, responde 'NO'.
    Recuerda que cualquier dato que sea un porcentaje o que pueda traducirse a un porcentaje se considera estadistico.
    **IMPORTANTE**
    Responde unicamente con SI o NO nada mas.
    Consulta: {prompt}
    """
    
    try:
        response = client.chat.completions.create(
            model="bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0",
            messages=[{"role": "user", "content": analysis_prompt}]
        )
        analysis_result = response.choices[0].message.content.strip()
        print(analysis_result)
        return analysis_result.upper() == "SI"
    except Exception as e:
        print(f"Error en el an√°lisis del prompt: {e}")
        return False

# Funci√≥n para generar datos estad√≠sticos con base en la consulta del usuario
# Si Claude detecta que se requieren estad√≠sticas, esta funci√≥n genera datos estructurados en JSON
def generate_statistics_data(prompt):
    """
    Usa Claude para generar datos estad√≠sticos con base en la pregunta del usuario.
    """
    stats_prompt = f"""
    Genera datos estad√≠sticos relevantes en formato JSON para responder la siguiente consulta, quiero que busques 
    informacion real sobre los datos estadisticos exactos que vas a generar.
    {prompt}
    
    Devuelve una lista de datos num√©ricos estructurados de la siguiente forma:
    {{ "categorias": ["Categoria1", "Categoria2"], "valores": [10, 20] }}
    Aseg√∫rate de devolver solo un JSON v√°lido sin explicaciones adicionales.
    """
    
    try:
        response = client.chat.completions.create(
            model="bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0",
            messages=[{"role": "user", "content": stats_prompt}]
        )
        
        stats_text = response.choices[0].message.content.strip()
        print("Respuesta de Claude para estad√≠sticas:", stats_text)  # Debugging
        
        if not stats_text:
            raise ValueError("Claude no devolvi√≥ una respuesta v√°lida para las estad√≠sticas.")
        
        stats_data = json.loads(stats_text)
        return stats_data
    except json.JSONDecodeError as e:
        print(f"Error al decodificar JSON: {e}")
        return None
    except Exception as e:
        print(f"Error en la generaci√≥n de estad√≠sticas: {e}")
        return None


    
# Funci√≥n para generar un gr√°fico interactivo con Plotly a partir de los datos obtenidos
def plot_statistics(stats_data):
    """
    Genera un gr√°fico interactivo con Plotly basado en los datos obtenidos.
    """
    if not stats_data or "categorias" not in stats_data or "valores" not in stats_data:
        return None
    
    fig = px.bar(x=stats_data["categorias"], y=stats_data["valores"],
                 labels={'x': 'Categor√≠as', 'y': 'Valores'},
                 title="Datos Estad√≠sticos Generados",
                 color=stats_data["valores"],
                 color_continuous_scale="viridis")
    
    return fig
def a√±adirEnlances_ChatGPT(promptInput):

    prompt = f"""Eres un agente especializado en proveer art√≠culos de pubMed (https://pubmed.ncbi.nlm.nih.gov/). Tu funci√≥n es proveer enlaces
    relevantes a art√≠culos para este texto generado por un asistente m√©dico de un doctor: {promptInput}, 
    """

    completion = client_PEDRO.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
            "role": "assistant",
            "content": prompt,
            }
        ]
    )   
    response_text = completion.choices[0].message.content if completion.choices else "No se encontraron enlaces relevantes."
    #print(response_text)
    return response_text
    

    #Este agente ya devuelve los enlaces sin errores a pubMed
    #Hace falta a√±adirlo a continuaci√≥n de process_chat_message en el prompt
   

#Este m√©todo debe llamarse al seleccionar un paciente, a modo introductorio, se le pasa la informaci√≥n resumen del paciente y un link de aumentaci√≥n de datos.
#IDEA si Claude.puede leer p√°ginas web subir la informaci√≥n a una p√°gina para no consumir tokens
def returnPatientSummary(idioma, selected_patient, userName):
    json_resumen_paciente = json_info_from_instance_class(selected_patient)
    prompt = f"""Traduce la respuesta al idioma seleccionado: {idioma}.
    Solo da la respuesta en el idioma que te he pedido.
    Eres un m√©dico profesional, quiero que respondas con un vocabulario t√©cnico
    y a√±adas informaci√≥n relevante a la consulta.
    
    Debes saludar al Dr.{userName} y hacer un resumen con la informaci√≥n del paciente {json_resumen_paciente}.
    Termina la respuesta pidiedo al Dr. M√°s instrucciones para continuar
    """
    try:
        response = client.chat.completions.create(
            model="bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0",
            messages=[{"role": "user", "content": prompt}] ,
            stream=True  
        )
        buffer = []
        for chunk in response:
            if hasattr(chunk, "choices") and chunk.choices:
                chunk_text = chunk.choices[0].delta.content or ""
                buffer.append(chunk_text)

                # Control de latencia din√°mico seg√∫n la cantidad de chunks
                if len(buffer) % 5 == 0:
                    time.sleep(0.02)
                
                yield "".join(buffer)
                buffer.clear()

    except Exception as e:
        print(f"Error en la API: {e}")
        return "Lo sentimos, nuestro LLM no est√° disponible en este momento, por favor int√©ntelo m√°s tarde."
    
    


def process_chat_message(prompt, idioma, file_context, conver_history, selected_patient):

    patient_json_info = all_patient_info(selected_patient.PacienteID)

    """Procesa el mensaje del usuario y genera respuesta del LLM, incluyendo los 6 CSVs autom√°ticamente."""

    if conver_history is None:
        conver_history = []
    # Convertir info paciente a JSON legible para Claude, informaci√≥n resumen o completa (puede variar dependiendo del prompt)
    #patient_json_info = json_info_from_instance_class(selected_patient)
    #patient_json_all_info =  all_patient_info(selected_patient.PatientID) (int)
    #M√©todo all_patient_info()
    
    preprompt = f"""Traduce la respuesta al idioma seleccionado: {idioma}.
    Solo da la respuesta en el idioma que te he pedido.
    Eres un m√©dico profesional, quiero que respondas con un vocabulario t√©cnico
    y a√±adas informaci√≥n relevante a la consulta.
    

    üìÑ **Contexto de archivos adjuntos**:
    {file_context}

    üìä **Datos estructurados de la informaci√≥n cl√≠nica del paciente al ingresar en el centro de salud**:
    {patient_json_info}

    Debes comprender la informaci√≥n proporcionada para poder responder correctamente a las preguntas
    que se te impone, puedes cumplimentar tus observaciones con tablas estadisticas sobre las enfermedades 
    comentadas.

    **Pregunta del usuario**:
    {prompt}
    """

    # Convertir historial a JSON estructurado
    history_messages = [{"role": "user", "content": msg["content"]} for msg in conver_history]
    
    try:
        response = client.chat.completions.create(
            model="bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0",
            messages=[{"role": "user", "content": preprompt}] + history_messages,
            stream=True  
        )
        buffer = []
        for chunk in response:
            if hasattr(chunk, "choices") and chunk.choices:
                chunk_text = chunk.choices[0].delta.content or ""
                buffer.append(chunk_text)

                # Control de latencia din√°mico seg√∫n la cantidad de chunks
                if len(buffer) % 5 == 0:
                    time.sleep(0.02)
                
                yield "".join(buffer)
                buffer.clear()

    except Exception as e:
        print(f"Error en la API: {e}")
        return "Lo sentimos, nuestro LLM no est√° disponible en este momento, por favor int√©ntelo m√°s tarde."
    