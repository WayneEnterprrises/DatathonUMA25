from .config import client
import time
import json

from DB.dbInterface import *


#Este m√©todo debe llamarse al seleccionar un paciente, a modo introductorio, se le pasa la informaci√≥n resumen del paciente y un link de aumentaci√≥n de datos.
#IDEA si Claude.puede leer p√°ginas web subir la informaci√≥n a una p√°gina para no consumir tokens
def returnPatientSummary(idioma, selected_patient, userName):
    json_resumen_paciente = json_info_from_instance_class(selected_patient)
    prompt = f"""Traduce la respuesta al idioma seleccionado: {idioma}.
    Solo da la respuesta en el idioma que te he pedido.
    Eres un m√©dico profesional, quiero que respondas con un vocabulario t√©cnico
    y a√±adas informaci√≥n relevante a la consulta, a√±ade enlaces de interes sobre las enfermedades que se traten en la conversacion.
    
    Debes saludar al Dr.{userName} y hacer un resumen con la informaci√≥n del paciente {json_resumen_paciente}.
    Termina la respuesta pidiedo al Dr. M√°s instrucciones para continuar
    """
    try:
        response = client.chat.completions.create(
            model="bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0",
            messages=[{"role": "user", "content": prompt}] ,
            stream=True  
        )
        buffer = []
        for chunk in response:
            if hasattr(chunk, "choices") and chunk.choices:
                chunk_text = chunk.choices[0].delta.content or ""
                buffer.append(chunk_text)

                # Control de latencia din√°mico seg√∫n la cantidad de chunks
                if len(buffer) % 5 == 0:
                    time.sleep(0.02)
                
                yield "".join(buffer)
                buffer.clear()

    except Exception as e:
        print(f"Error en la API: {e}")
        return "Lo sentimos, nuestro LLM no est√° disponible en este momento, por favor int√©ntelo m√°s tarde."
    
    


def process_chat_message(prompt, idioma, file_context, conver_history, selected_patient):

    patient_json_info = json_info_from_instance_class(selected_patient)

    """Procesa el mensaje del usuario y genera respuesta del LLM, incluyendo los 6 CSVs autom√°ticamente."""

    if conver_history is None:
        conver_history = []
    # Convertir info paciente a JSON legible para Claude, informaci√≥n resumen o completa (puede variar dependiendo del prompt)
    #patient_json_info = json_info_from_instance_class(selected_patient)
    #patient_json_all_info =  all_patient_info(selected_patient.PatientID) (int)
    #M√©todo all_patient_info()
    
    preprompt = f"""Traduce la respuesta al idioma seleccionado: {idioma}.
    Solo da la respuesta en el idioma que te he pedido.
    Eres un m√©dico profesional, quiero que respondas con un vocabulario t√©cnico
    y a√±adas informaci√≥n relevante a la consulta, a√±ade enlaces de interes sobre las enfermedades que se traten en la conversacion.
    

    üìÑ **Contexto de archivos adjuntos**:
    {file_context}

    üìä **Datos estructurados de la informaci√≥n cl√≠nica del paciente al ingresar en el centro de salud**:
    {patient_json_info}

    Debes comprender la informaci√≥n proporcionada para poder responder correctamente a las preguntas
    que se te impone, puedes cumplimentar tus observaciones con tablas estadisticas sobre las enfermedades 
    comentadas.

    **Pregunta del usuario**:
    {prompt}
    """

    # Convertir historial a JSON estructurado
    history_messages = [{"role": "user", "content": msg["content"]} for msg in conver_history]
    
    try:
        response = client.chat.completions.create(
            model="bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0",
            messages=[{"role": "user", "content": preprompt}] + history_messages,
            stream=True  
        )

        buffer = []
        for chunk in response:
            if hasattr(chunk, "choices") and chunk.choices:
                chunk_text = chunk.choices[0].delta.content or ""
                buffer.append(chunk_text)

                # Control de latencia din√°mico seg√∫n la cantidad de chunks
                if len(buffer) % 5 == 0:
                    time.sleep(0.02)
                
                yield "".join(buffer)
                buffer.clear()

    except Exception as e:
        print(f"Error en la API: {e}")
        return "Lo sentimos, nuestro LLM no est√° disponible en este momento, por favor int√©ntelo m√°s tarde."