from .config import client
import time

def process_chat_message(prompt, idioma, file_context):
    """üîÑ Genera la respuesta del LLM en tiempo real"""
    preprompt = f"Traduce la respuesta al idioma seleccionado: {idioma}. \
    Solo da la respuesta en el idioma que te he pedido. \
    Eres un m√©dico profesional, quiero que respondas con un vocabulario t√©cnico \
    y a√±adas informaci√≥n relevante a la consulta."

    complete_prompt = f"""üìÑ **Contexto de archivos adjuntos**:
    {file_context}

    **Pregunta del usuario**:
    {prompt}
    """

    try:
        response = client.chat.completions.create(
            model="bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0",
            messages=[{"role": "user", "content": preprompt + complete_prompt}],
            stream=True 
        )

        for chunk in response:
            if hasattr(chunk, "choices") and chunk.choices:
                yield chunk.choices[0].delta.content or ""
            time.sleep(0.05)  

    except Exception:
        yield "Lo sentimos, nuestro LLM no est√° disponible en este momento, por favor int√©ntelo m√°s tarde."
